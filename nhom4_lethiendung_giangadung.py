# -*- coding: utf-8 -*-
"""Nhom4_LETHIENDUNG_GIANGADUNG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhqaC9eXANdz2vzSFaS7qUEyp2-LeV-v
"""

# ========================
# TR·ª∞C QUAN H√ìA
# ========================

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")

print("üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu:", df.shape)

# 2. X√°c ƒë·ªãnh c·ªôt nh√£n
target_col = "category"   # ‚ö†Ô∏è ƒë·ªïi n·∫øu c·ªôt nh√£n c√≥ t√™n kh√°c

if target_col in df.columns:
    label_counts = df[target_col].value_counts()

    print("\nüîπ T·ªïng s·ªë nh√£n:", df[target_col].nunique())
    print("üîπ T·ªïng s·ªë m·∫´u:", df.shape[0])

    # L·∫•y top 10 nh√£n nhi·ªÅu m·∫´u nh·∫•t
    top_n = 10
    top_labels = label_counts.head(top_n)

    print(f"\nüìå Top {top_n} nh√£n c√≥ nhi·ªÅu m·∫´u nh·∫•t:")
    print(top_labels)

    # 3. Bi·ªÉu ƒë·ªì c·ªôt
    plt.figure(figsize=(10,5))
    sns.barplot(x=top_labels.index, y=top_labels.values, palette="viridis")
    plt.title(f"S·ªë l∆∞·ª£ng m·∫´u theo Top {top_n} nh√£n")
    plt.ylabel("S·ªë m·∫´u")
    plt.xlabel("Nh√£n")
    plt.xticks(rotation=45, ha="right")
    plt.show()
else:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y c·ªôt 'category'. Vui l√≤ng ki·ªÉm tra l·∫°i t√™n c·ªôt nh√£n trong d·ªØ li·ªáu!")

# ========================
# L√†m s·∫°ch d·ªØ li·ªáu
# HI·ªÇN TH·ªä D·ªÆ LI·ªÜU TR∆Ø·ªöC & SAU KHI L√ÄM S·∫†CH
# ========================

import pandas as pd

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")

print("üìä K√≠ch th∆∞·ªõc d·ªØ li·ªáu ban ƒë·∫ßu:", df.shape)

# 2. Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu ti√™n tr∆∞·ªõc khi l√†m s·∫°ch
print("\nüìå 10 d√≤ng d·ªØ li·ªáu ban ƒë·∫ßu (ch∆∞a l√†m s·∫°ch):")
print(df.head(10))

# 3. L√†m s·∫°ch d·ªØ li·ªáu
df_cleaned = df.dropna()              # X√≥a gi√° tr·ªã thi·∫øu
df_cleaned = df_cleaned.drop_duplicates()  # X√≥a tr√πng l·∫∑p
df_cleaned.columns = df_cleaned.columns.str.strip()  # Chu·∫©n h√≥a t√™n c·ªôt

print("\n‚úÖ K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi l√†m s·∫°ch:", df_cleaned.shape)

# 4. Hi·ªÉn th·ªã 10 d√≤ng ƒë·∫ßu ti√™n sau khi l√†m s·∫°ch
print("\nüìå 10 d√≤ng d·ªØ li·ªáu sau khi l√†m s·∫°ch:")
print(df_cleaned.head(10))

# ========================
# CHU·∫®N HO√Å D·ªÆ LI·ªÜU
# ========================

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder

# 1. Load v√† l√†m s·∫°ch d·ªØ li·ªáu tr∆∞·ªõc
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()   # X√≥a gi√° tr·ªã thi·∫øu v√† d√≤ng tr√πng l·∫∑p
df.columns = df.columns.str.strip()  # Chu·∫©n h√≥a t√™n c·ªôt

# 2. X√°c ƒë·ªãnh ƒë·∫∑c tr∆∞ng s·ªë (X) v√† nh√£n (y)
dac_trung_so = ['summary_word_count']   # ‚ö†Ô∏è N·∫øu c√≥ nhi·ªÅu c·ªôt s·ªë th√¨ th√™m v√†o ƒë√¢y

X = df[dac_trung_so]   # Ch·ªâ l·∫•y ƒë·∫∑c tr∆∞ng d·∫°ng s·ªë
y = df["category"]

# 3. M√£ h√≥a nh√£n n·∫øu l√† d·∫°ng text
if y.dtype == "object":
    le = LabelEncoder()
    y_mahoa = le.fit_transform(y)

# 4. Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng b·∫±ng StandardScaler
scaler = StandardScaler()
X_chuanhoa = scaler.fit_transform(X)

# 5. In 10 d√≤ng d·ªØ li·ªáu ban ƒë·∫ßu (ch∆∞a chu·∫©n h√≥a)
print("üìå 10 d√≤ng d·ªØ li·ªáu ban ƒë·∫ßu (ch∆∞a chu·∫©n h√≥a):")
print(X.head(10))

# 6. In 10 d√≤ng d·ªØ li·ªáu sau khi chu·∫©n h√≥a
print("\nüìå 10 d√≤ng d·ªØ li·ªáu sau khi chu·∫©n h√≥a:")
print(X_chuanhoa[:10])

# ========================
# SO S√ÅNH VECTOR H√ìA VƒÇN B·∫¢N
# ========================

import pandas as pd
import time
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split

# 1. Load v√† l√†m s·∫°ch d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()
df.columns = df.columns.str.strip()

# 2. Gi·ªõi h·∫°n s·ªë m·∫´u ƒë·ªÉ ch·∫°y nhanh h∆°n
df_sample = df.sample(2000, random_state=42)
texts = df_sample["summary"].astype(str).tolist()

# L∆∞u k·∫øt qu·∫£ so s√°nh
results = []

# 3. Bag-of-Words
start = time.time()
vectorizer_bow = CountVectorizer(max_features=5000)
X_bow = vectorizer_bow.fit_transform(texts)
end = time.time()
results.append(["Bag-of-Words", X_bow.shape, end-start, "Y·∫øu (m·∫•t ng·ªØ nghƒ©a)"])

# 4. TF-IDF
start = time.time()
vectorizer_tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer_tfidf.fit_transform(texts)
end = time.time()
results.append(["TF-IDF", X_tfidf.shape, end-start, "Trung b√¨nh (c√≥ tr·ªçng s·ªë, nh∆∞ng m·∫•t ng·ªØ c·∫£nh)"])

# 5. Sentence Embeddings
start = time.time()
model = SentenceTransformer('all-MiniLM-L6-v2')
X_embed = model.encode(texts, show_progress_bar=True)
end = time.time()
results.append(["Sentence Embeddings", X_embed.shape, end-start, "T·ªët (gi·ªØ ƒë∆∞·ª£c ng·ªØ nghƒ©a)"])

# 6. Xu·∫•t b·∫£ng so s√°nh
df_compare = pd.DataFrame(results, columns=["Ph∆∞∆°ng ph√°p", "K√≠ch th∆∞·ªõc ma tr·∫≠n", "Th·ªùi gian (gi√¢y)", "ƒê√°nh gi√°"])
print("\nüìä B·∫¢NG SO S√ÅNH C√ÅC VECTOR H√ìA\n")
print(df_compare)

# 7. Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t -> Sentence Embeddings
print("\n‚úÖ Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t: Sentence Embeddings (all-MiniLM-L6-v2)\n")

# 8. Chia t·∫≠p train/test ƒë·ªÉ s·ª≠ d·ª•ng v·ªÅ sau
X_train, X_test = train_test_split(X_embed, test_size=0.2, random_state=42)
print("üìå T·∫≠p train:", X_train.shape)
print("üìå T·∫≠p test :", X_test.shape)

# ========================
# THU·∫¨T TO√ÅN KNN
# ========================

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()
df.columns = df.columns.str.strip()

# 2. L·∫•y m·∫´u nh·ªè ƒë·ªÉ ch·∫°y nhanh
df_sample = df.sample(2000, random_state=42)

texts = df_sample["summary"].astype(str).tolist()
labels = df_sample["category"].astype(str).tolist()   # nh√£n l√† category

# 3. Sentence Embedding
model = SentenceTransformer('all-MiniLM-L6-v2')
X_embed = model.encode(texts, show_progress_bar=True)

# 4. Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(X_embed, labels, test_size=0.2, random_state=42)

# 5. Hu·∫•n luy·ªán KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# 6. D·ª± ƒëo√°n
y_pred = knn.predict(X_test)

# 7. ƒê√°nh gi√° m√¥ h√¨nh
print("\nüîπ K·∫øt qu·∫£ KNN:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 8. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n
cm = confusion_matrix(y_test, y_pred, labels=knn.classes_)
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=False, cmap="Blues", xticklabels=knn.classes_, yticklabels=knn.classes_)
plt.xlabel("D·ª± ƒëo√°n")
plt.ylabel("Th·ª±c t·∫ø")
plt.title("Ma tr·∫≠n nh·∫ßm l·∫´n - KNN")
plt.show()

# ========================
# THU·∫¨T TO√ÅN NAIVE BAYES
# ========================

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()
df.columns = df.columns.str.strip()

# 2. L·∫•y m·∫´u nh·ªè ƒë·ªÉ ch·∫°y nhanh
df_sample = df.sample(2000, random_state=42)

texts = df_sample["summary"].astype(str).tolist()
labels = df_sample["category"].astype(str).tolist()

# 3. Sentence Embedding
model = SentenceTransformer('all-MiniLM-L6-v2')
X_embed = model.encode(texts, show_progress_bar=True)

# 4. Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(X_embed, labels, test_size=0.2, random_state=42)

# 5. Hu·∫•n luy·ªán Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)

# 6. D·ª± ƒëo√°n
y_pred = nb.predict(X_test)

# 7. ƒê√°nh gi√° m√¥ h√¨nh
print("\nüîπ K·∫øt qu·∫£ Naive Bayes:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 8. Ch·ªâ l·∫•y top 10 nh√£n ph·ªï bi·∫øn nh·∫•t
top_labels = pd.Series(y_test).value_counts().nlargest(10).index.tolist()

y_test_filtered = ["Other" if lbl not in top_labels else lbl for lbl in y_test]
y_pred_filtered = ["Other" if lbl not in top_labels else lbl for lbl in y_pred]

# 9. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n g·ªçn
cm = confusion_matrix(y_test_filtered, y_pred_filtered, labels=top_labels+["Other"])
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=top_labels+["Other"],
            yticklabels=top_labels+["Other"])
plt.xlabel("D·ª± ƒëo√°n")
plt.ylabel("Th·ª±c t·∫ø")
plt.title("Ma tr·∫≠n nh·∫ßm l·∫´n (Top 10 l·ªõp ch√≠nh + Other) - Naive Bayes")
plt.show()

# ========================
# THU·∫¨T TO√ÅN C√ÇY QUY·∫æT ƒê·ªäNH
# ========================

import pandas as pd
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()
df.columns = df.columns.str.strip()

# 2. L·∫•y m·∫´u nh·ªè ƒë·ªÉ ch·∫°y nhanh
df_sample = df.sample(2000, random_state=42)

texts = df_sample["summary"].astype(str).tolist()
labels = df_sample["category"].astype(str).tolist()

# 3. Sentence Embedding
model = SentenceTransformer('all-MiniLM-L6-v2')
X_embed = model.encode(texts, show_progress_bar=True)

# 4. Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_embed, labels, test_size=0.2, random_state=42
)

# 5. Hu·∫•n luy·ªán Decision Tree
dt = DecisionTreeClassifier(random_state=42, max_depth=20)  # gi·ªõi h·∫°n ƒë·ªô s√¢u ƒë·ªÉ tr√°nh overfitting
dt.fit(X_train, y_train)

# 6. D·ª± ƒëo√°n
y_pred = dt.predict(X_test)

# 7. ƒê√°nh gi√° m√¥ h√¨nh
print("\nüîπ K·∫øt qu·∫£ Decision Tree:")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 8. Ch·ªâ l·∫•y top 10 nh√£n ph·ªï bi·∫øn nh·∫•t
top_labels = pd.Series(y_test).value_counts().nlargest(10).index.tolist()

y_test_filtered = ["Other" if lbl not in top_labels else lbl for lbl in y_test]
y_pred_filtered = ["Other" if lbl not in top_labels else lbl for lbl in y_pred]

# 9. V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n g·ªçn
cm = confusion_matrix(y_test_filtered, y_pred_filtered, labels=top_labels+["Other"])
plt.figure(figsize=(10,7))
sns.heatmap(cm, annot=True, fmt="d", cmap="Oranges",
            xticklabels=top_labels+["Other"],
            yticklabels=top_labels+["Other"])
plt.xlabel("D·ª± ƒëo√°n")
plt.ylabel("Th·ª±c t·∫ø")
plt.title("Ma tr·∫≠n nh·∫ßm l·∫´n (Top 10 l·ªõp ch√≠nh + Other) - Decision Tree")
plt.show()

# ========================
# THU·∫¨T TO√ÅN K-MEANS
# ========================

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from sentence_transformers import SentenceTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load d·ªØ li·ªáu
df = pd.read_csv("arXiv_scientific dataset.csv")
df = df.dropna().drop_duplicates()
df.columns = df.columns.str.strip()

# 2. L·∫•y m·∫´u nh·ªè ƒë·ªÉ ch·∫°y nhanh
df_sample = df.sample(2000, random_state=42)

texts = df_sample["summary"].astype(str).tolist()
labels = df_sample["category"].astype(str).tolist()

# 3. Sentence Embedding
model = SentenceTransformer('all-MiniLM-L6-v2')
X_embed = model.encode(texts, show_progress_bar=True)

# 4. Hu·∫•n luy·ªán K-Means
num_clusters = len(set(labels))  # s·ªë c·ª•m = s·ªë nh√£n th·ª±c
kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
y_pred = kmeans.fit_predict(X_embed)

# 5. ƒê√°nh gi√° b·∫±ng ch·ªâ s·ªë clustering
ari = adjusted_rand_score(labels, y_pred)
nmi = normalized_mutual_info_score(labels, y_pred)

print("\nüîπ K·∫øt qu·∫£ K-Means:")
print("Adjusted Rand Index (ARI):", ari)
print("Normalized Mutual Information (NMI):", nmi)

# 6. V·∫Ω ph√¢n b·ªë c·ª•m theo nh√£n th·∫≠t (top 10 nh√£n)
df_vis = pd.DataFrame({"True_Label": labels, "Cluster": y_pred})
top_labels = df_vis["True_Label"].value_counts().nlargest(10).index.tolist()
df_vis_filtered = df_vis[df_vis["True_Label"].isin(top_labels)]

plt.figure(figsize=(10,6))
sns.countplot(data=df_vis_filtered, x="Cluster", hue="True_Label", palette="tab10")
plt.title("Ph√¢n b·ªë c·ª•m theo nh√£n th·∫≠t (Top 10 nh√£n) - KMeans")
plt.xlabel("C·ª•m KMeans")
plt.ylabel("S·ªë l∆∞·ª£ng m·∫´u")
plt.legend(title="Nh√£n th·∫≠t", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()

import matplotlib.pyplot as plt

# K·∫øt qu·∫£ m√¥ h√¨nh (b·∫°n thay s·ªë Accuracy c·ªßa Decision Tree cho ƒë√∫ng th·ª±c t·∫ø)
results = {
    "KNN": 0.67,
    "Naive Bayes": 0.645,
    "Decision Tree": 0.66,
    "K-Means (NMI)": 0.374
}

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8,5))
bars = plt.bar(results.keys(), results.values(),
               color=["#4CAF50","#2196F3","#FF9800","#9C27B0"],
               width=0.6, edgecolor="black", linewidth=1)

# Th√™m gi√° tr·ªã tr√™n c·ªôt
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.015,
             f"{yval:.3f}", ha='center', fontsize=11, fontweight="bold")

# T√πy ch·ªânh tr·ª•c
plt.ylabel("ƒê·ªô ch√≠nh x√°c / Ch·ªâ s·ªë NMI", fontsize=12)
plt.title("So s√°nh hi·ªáu qu·∫£ c√°c m√¥ h√¨nh h·ªçc m√°y", fontsize=14, fontweight="bold")
plt.ylim(0,1)
plt.grid(axis="y", linestyle="--", alpha=0.7)

plt.show()